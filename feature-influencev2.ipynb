{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PassengerId</th>\n",
       "      <th>Survived</th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Name</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Age</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Ticket</th>\n",
       "      <th>Fare</th>\n",
       "      <th>Cabin</th>\n",
       "      <th>Embarked</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Braund, Mr. Owen Harris</td>\n",
       "      <td>male</td>\n",
       "      <td>22.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>A/5 21171</td>\n",
       "      <td>7.2500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Cumings, Mrs. John Bradley (Florence Briggs Th...</td>\n",
       "      <td>female</td>\n",
       "      <td>38.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>PC 17599</td>\n",
       "      <td>71.2833</td>\n",
       "      <td>C85</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>Heikkinen, Miss. Laina</td>\n",
       "      <td>female</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>STON/O2. 3101282</td>\n",
       "      <td>7.9250</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Futrelle, Mrs. Jacques Heath (Lily May Peel)</td>\n",
       "      <td>female</td>\n",
       "      <td>35.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>113803</td>\n",
       "      <td>53.1000</td>\n",
       "      <td>C123</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Allen, Mr. William Henry</td>\n",
       "      <td>male</td>\n",
       "      <td>35.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>373450</td>\n",
       "      <td>8.0500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   PassengerId  Survived  Pclass  \\\n",
       "0            1         0       3   \n",
       "1            2         1       1   \n",
       "2            3         1       3   \n",
       "3            4         1       1   \n",
       "4            5         0       3   \n",
       "\n",
       "                                                Name     Sex   Age  SibSp  \\\n",
       "0                            Braund, Mr. Owen Harris    male  22.0      1   \n",
       "1  Cumings, Mrs. John Bradley (Florence Briggs Th...  female  38.0      1   \n",
       "2                             Heikkinen, Miss. Laina  female  26.0      0   \n",
       "3       Futrelle, Mrs. Jacques Heath (Lily May Peel)  female  35.0      1   \n",
       "4                           Allen, Mr. William Henry    male  35.0      0   \n",
       "\n",
       "   Parch            Ticket     Fare Cabin Embarked  \n",
       "0      0         A/5 21171   7.2500   NaN        S  \n",
       "1      0          PC 17599  71.2833   C85        C  \n",
       "2      0  STON/O2. 3101282   7.9250   NaN        S  \n",
       "3      0            113803  53.1000  C123        S  \n",
       "4      0            373450   8.0500   NaN        S  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "\n",
    "from influence.dataset import DataSet\n",
    "import tensorflow as tf\n",
    "from tensorflow.contrib.learn.python.learn.datasets import base\n",
    "\n",
    "\n",
    "fpath_titanic = \"/home/eolus/Desktop/Dauphine/datamining/projets/blackBox/data/train.csv\"\n",
    "train_df = pd.read_csv(fpath_titanic)\n",
    "\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def extract_prefix(name):\n",
    "    import re\n",
    "    try:\n",
    "        return re.search('(Mr\\.)|(Mrs\\.)|(Miss\\.)', name).group()\n",
    "    except:\n",
    "        return \"\"\n",
    "\n",
    "\n",
    "train_df['Prefix'] = train_df.Name.apply(extract_prefix)\n",
    "\n",
    "for cat_col in ['Sex', 'Embarked', 'Prefix' ]:\n",
    "    train_df[cat_col] = pd.factorize(train_df[cat_col])[0]\n",
    "    \n",
    "train_df['Age'].fillna(train_df.Age.mean(), inplace=True)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "features = ['Prefix', 'Pclass', 'Sex', 'Age', 'SibSp', 'Parch', 'Fare', 'Embarked']\n",
    "\n",
    "X = np.array(train_df[features])\n",
    "y = np.array((train_df.Survived > 0).astype('int32'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Scale\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(X)\n",
    "\n",
    "# Need to train/test split\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_model(X_train, X_test, y_train, y_test):\n",
    "    \n",
    "    # Create dataset objects\n",
    "    import sys\n",
    "    sys.path.append(\"..\")\n",
    "\n",
    "    #from influence indataset as dataset\n",
    "    from influence.dataset import DataSet\n",
    "    import numpy as np\n",
    "    lr_train = DataSet(X_train, np.array(y_train, dtype=int))\n",
    "    lr_test = DataSet(X_test, np.array(y_test, dtype=int))\n",
    "    lr_validation = None\n",
    "\n",
    "    import tensorflow as tf\n",
    "    from tensorflow.contrib.learn.python.learn.datasets import base\n",
    "    lr_data_sets = base.Datasets(train=lr_train, validation=lr_validation, test=lr_test)\n",
    "\n",
    "    from influence.binaryLogisticRegressionWithLBFGS import BinaryLogisticRegressionWithLBFGS\n",
    "\n",
    "    num_classes = 2\n",
    "    #input_dim = len(features)\n",
    "    input_dim = len(X_test.T)\n",
    "\n",
    "    weight_decay = 0.01\n",
    "    batch_size = 100\n",
    "    initial_learning_rate = 0.001 \n",
    "    keep_probs = None\n",
    "    decay_epochs = [1000, 10000]\n",
    "    max_lbfgs_iter = 1000\n",
    "\n",
    "    tf.reset_default_graph()\n",
    "\n",
    "    tf_model = BinaryLogisticRegressionWithLBFGS(\n",
    "        input_dim=input_dim,\n",
    "        weight_decay=weight_decay,\n",
    "        max_lbfgs_iter=max_lbfgs_iter,\n",
    "        num_classes=num_classes, \n",
    "        batch_size=batch_size,\n",
    "        data_sets=lr_data_sets,\n",
    "        initial_learning_rate=initial_learning_rate,\n",
    "        keep_probs=keep_probs,\n",
    "        decay_epochs=decay_epochs,\n",
    "        mini_batch=False,\n",
    "        train_dir='tmp',\n",
    "        log_dir='tmp',\n",
    "        model_name='titanic')\n",
    "\n",
    "    tf_model.train()\n",
    "    return tf_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot(x, y):\n",
    "    import matplotlib.pyplot as plt\n",
    "    plt.plot( x, y, '-')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# Get top influential train points (I_up_loss)\n",
    "def get_top_train_influence(tf_model):\n",
    "    \"\"\"\n",
    "    Approximate most influential train points for a test point\n",
    "    idx : index of test point\n",
    "    \"\"\"\n",
    "    num_train = len(tf_model.data_sets.train.labels)\n",
    "    influences = tf_model.get_influence_on_test_loss(\n",
    "        np.arange(len(tf_model.data_sets.test.labels)), \n",
    "        np.arange(len(tf_model.data_sets.train.labels)),\n",
    "        force_refresh=True) * num_train\n",
    "    influences_sorted = sorted(enumerate(influences),\n",
    "                               key=lambda x:x[1],\n",
    "                               reverse=True)\n",
    "    return influences_sorted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def retrain_most_influential_train(top_train_influence):\n",
    "\n",
    "    scores = []\n",
    "    for i in range(3, 101, 1):\n",
    "\n",
    "        print(\"WITH {}% MOST INFLUENTIALTRAINING POINTS:\".format(i))\n",
    "        \n",
    "        top_n = [f[0] for f in top_train_influence[:int(i*len(top_train_influence)/100)]]\n",
    "        print(\"TOP N:\", len(top_n))\n",
    "        \n",
    "        from influence.dataset import DataSet\n",
    "        lr_train = DataSet(X_train[top_n,:], np.array(y_train[top_n], dtype=int))\n",
    "        lr_test = DataSet(X_test, np.array(y_test, dtype=int))\n",
    "        lr_validation = None\n",
    "        lr_data_sets = base.Datasets(train=lr_train, validation=lr_validation, test=lr_test)\n",
    "\n",
    "\n",
    "        from influence.binaryLogisticRegressionWithLBFGS import BinaryLogisticRegressionWithLBFGS\n",
    "\n",
    "        num_classes = 2\n",
    "        input_dim = len(X_train.T)\n",
    "\n",
    "        weight_decay = 0.01\n",
    "        batch_size = 100\n",
    "        initial_learning_rate = 0.001 \n",
    "        keep_probs = None\n",
    "        decay_epochs = [1000, 10000]\n",
    "        max_lbfgs_iter = 1000\n",
    "\n",
    "        tf.reset_default_graph()\n",
    "\n",
    "        tf_model = BinaryLogisticRegressionWithLBFGS(\n",
    "            input_dim=input_dim,\n",
    "            weight_decay=weight_decay,\n",
    "            max_lbfgs_iter=max_lbfgs_iter,\n",
    "            num_classes=num_classes, \n",
    "            batch_size=batch_size,\n",
    "            data_sets=lr_data_sets,\n",
    "            initial_learning_rate=initial_learning_rate,\n",
    "            keep_probs=keep_probs,\n",
    "            decay_epochs=decay_epochs,\n",
    "            mini_batch=False,\n",
    "            train_dir='tmp',\n",
    "            log_dir='tmp',\n",
    "            model_name='titanic')\n",
    "\n",
    "        tf_model.train()\n",
    "\n",
    "        preds_p = tf_model.get_preds().tolist()\n",
    "        preds = [1 if el[0] < 0.5 else 0 for el in preds_p]\n",
    "        ref = tf_model.data_sets.test.labels\n",
    "\n",
    "        same = 0\n",
    "        for test_point in zip(preds, ref):\n",
    "            if test_point[0] == test_point[1]:\n",
    "                same += 1\n",
    "        score = same / len(ref)\n",
    "        print(\"SCORE = \", score)\n",
    "        print()\n",
    "\n",
    "        scores.append((i, score))\n",
    "        \n",
    "    x = [s[0] for s in scores]\n",
    "    y = [s[1] for s in scores]\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#top_train_influence = get_top_train_influence(tf_model)\n",
    "#x, y = retrain_most_influential_train(top_train_influence)\n",
    "#plot(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of parameters: 3\n",
      "Using normal model\n",
      "LBFGS training took [7] iter.\n",
      "After training with LBFGS: \n",
      "Optimization terminated successfully.\n",
      "         Current function value: -0.007806\n",
      "         Iterations: 4\n",
      "         Function evaluations: 5\n",
      "         Gradient evaluations: 8\n",
      "         Hessian evaluations: 7\n",
      "WITH 3% MOST INFLUENTIALTRAINING POINTS:\n",
      "TOP N: 17\n",
      "Total number of parameters: 3\n",
      "Using normal model\n",
      "LBFGS training took [11] iter.\n",
      "After training with LBFGS: \n",
      "SCORE =  0.8033898305084746\n",
      "\n",
      "WITH 4% MOST INFLUENTIALTRAINING POINTS:\n",
      "TOP N: 23\n",
      "Total number of parameters: 3\n",
      "Using normal model\n",
      "LBFGS training took [11] iter.\n",
      "After training with LBFGS: \n",
      "SCORE =  0.8033898305084746\n",
      "\n",
      "WITH 5% MOST INFLUENTIALTRAINING POINTS:\n",
      "TOP N: 29\n",
      "Total number of parameters: 3\n",
      "Using normal model\n",
      "LBFGS training took [11] iter.\n",
      "After training with LBFGS: \n",
      "SCORE =  0.8033898305084746\n",
      "\n",
      "WITH 6% MOST INFLUENTIALTRAINING POINTS:\n",
      "TOP N: 35\n",
      "Total number of parameters: 3\n",
      "Using normal model\n",
      "LBFGS training took [11] iter.\n",
      "After training with LBFGS: \n",
      "SCORE =  0.8033898305084746\n",
      "\n",
      "WITH 7% MOST INFLUENTIALTRAINING POINTS:\n",
      "TOP N: 41\n",
      "Total number of parameters: 3\n",
      "Using normal model\n",
      "LBFGS training took [10] iter.\n",
      "After training with LBFGS: \n",
      "SCORE =  0.8033898305084746\n",
      "\n",
      "WITH 8% MOST INFLUENTIALTRAINING POINTS:\n",
      "TOP N: 47\n",
      "Total number of parameters: 3\n",
      "Using normal model\n",
      "LBFGS training took [11] iter.\n",
      "After training with LBFGS: \n",
      "SCORE =  0.8033898305084746\n",
      "\n",
      "WITH 9% MOST INFLUENTIALTRAINING POINTS:\n",
      "TOP N: 53\n",
      "Total number of parameters: 3\n",
      "Using normal model\n",
      "LBFGS training took [10] iter.\n",
      "After training with LBFGS: \n",
      "SCORE =  0.8033898305084746\n",
      "\n",
      "WITH 10% MOST INFLUENTIALTRAINING POINTS:\n",
      "TOP N: 59\n",
      "Total number of parameters: 3\n",
      "Using normal model\n",
      "LBFGS training took [10] iter.\n",
      "After training with LBFGS: \n",
      "SCORE =  0.8033898305084746\n",
      "\n",
      "WITH 11% MOST INFLUENTIALTRAINING POINTS:\n",
      "TOP N: 65\n",
      "Total number of parameters: 3\n",
      "Using normal model\n",
      "LBFGS training took [10] iter.\n",
      "After training with LBFGS: \n",
      "SCORE =  0.8033898305084746\n",
      "\n",
      "WITH 12% MOST INFLUENTIALTRAINING POINTS:\n",
      "TOP N: 71\n",
      "Total number of parameters: 3\n",
      "Using normal model\n",
      "LBFGS training took [10] iter.\n",
      "After training with LBFGS: \n",
      "SCORE =  0.8033898305084746\n",
      "\n",
      "WITH 13% MOST INFLUENTIALTRAINING POINTS:\n",
      "TOP N: 77\n",
      "Total number of parameters: 3\n",
      "Using normal model\n",
      "LBFGS training took [10] iter.\n",
      "After training with LBFGS: \n",
      "SCORE =  0.8033898305084746\n",
      "\n",
      "WITH 14% MOST INFLUENTIALTRAINING POINTS:\n",
      "TOP N: 83\n",
      "Total number of parameters: 3\n",
      "Using normal model\n",
      "LBFGS training took [10] iter.\n",
      "After training with LBFGS: \n",
      "SCORE =  0.8033898305084746\n",
      "\n",
      "WITH 15% MOST INFLUENTIALTRAINING POINTS:\n",
      "TOP N: 89\n",
      "Total number of parameters: 3\n",
      "Using normal model\n",
      "LBFGS training took [10] iter.\n",
      "After training with LBFGS: \n",
      "SCORE =  0.8033898305084746\n",
      "\n",
      "WITH 16% MOST INFLUENTIALTRAINING POINTS:\n",
      "TOP N: 95\n",
      "Total number of parameters: 3\n",
      "Using normal model\n",
      "LBFGS training took [10] iter.\n",
      "After training with LBFGS: \n",
      "SCORE =  0.8033898305084746\n",
      "\n",
      "WITH 17% MOST INFLUENTIALTRAINING POINTS:\n",
      "TOP N: 101\n",
      "Total number of parameters: 3\n",
      "Using normal model\n",
      "LBFGS training took [10] iter.\n",
      "After training with LBFGS: \n",
      "SCORE =  0.8033898305084746\n",
      "\n",
      "WITH 18% MOST INFLUENTIALTRAINING POINTS:\n",
      "TOP N: 107\n",
      "Total number of parameters: 3\n",
      "Using normal model\n",
      "LBFGS training took [10] iter.\n",
      "After training with LBFGS: \n",
      "SCORE =  0.8033898305084746\n",
      "\n",
      "WITH 19% MOST INFLUENTIALTRAINING POINTS:\n",
      "TOP N: 113\n",
      "Total number of parameters: 3\n",
      "Using normal model\n",
      "LBFGS training took [10] iter.\n",
      "After training with LBFGS: \n",
      "SCORE =  0.8033898305084746\n",
      "\n",
      "WITH 20% MOST INFLUENTIALTRAINING POINTS:\n",
      "TOP N: 119\n",
      "Total number of parameters: 3\n",
      "Using normal model\n",
      "LBFGS training took [10] iter.\n",
      "After training with LBFGS: \n",
      "SCORE =  0.8033898305084746\n",
      "\n",
      "WITH 21% MOST INFLUENTIALTRAINING POINTS:\n",
      "TOP N: 125\n",
      "Total number of parameters: 3\n",
      "Using normal model\n",
      "LBFGS training took [10] iter.\n",
      "After training with LBFGS: \n",
      "SCORE =  0.8033898305084746\n",
      "\n",
      "WITH 22% MOST INFLUENTIALTRAINING POINTS:\n",
      "TOP N: 131\n",
      "Total number of parameters: 3\n",
      "Using normal model\n",
      "LBFGS training took [10] iter.\n",
      "After training with LBFGS: \n",
      "SCORE =  0.8033898305084746\n",
      "\n",
      "WITH 23% MOST INFLUENTIALTRAINING POINTS:\n",
      "TOP N: 137\n",
      "Total number of parameters: 3\n",
      "Using normal model\n",
      "LBFGS training took [11] iter.\n",
      "After training with LBFGS: \n",
      "SCORE =  0.8033898305084746\n",
      "\n",
      "WITH 24% MOST INFLUENTIALTRAINING POINTS:\n",
      "TOP N: 143\n",
      "Total number of parameters: 3\n",
      "Using normal model\n",
      "LBFGS training took [11] iter.\n",
      "After training with LBFGS: \n",
      "SCORE =  0.8033898305084746\n",
      "\n",
      "WITH 25% MOST INFLUENTIALTRAINING POINTS:\n",
      "TOP N: 149\n",
      "Total number of parameters: 3\n",
      "Using normal model\n",
      "LBFGS training took [11] iter.\n",
      "After training with LBFGS: \n",
      "SCORE =  0.8033898305084746\n",
      "\n",
      "WITH 26% MOST INFLUENTIALTRAINING POINTS:\n",
      "TOP N: 154\n",
      "Total number of parameters: 3\n",
      "Using normal model\n",
      "LBFGS training took [11] iter.\n",
      "After training with LBFGS: \n",
      "SCORE =  0.8033898305084746\n",
      "\n",
      "WITH 27% MOST INFLUENTIALTRAINING POINTS:\n",
      "TOP N: 160\n",
      "Total number of parameters: 3\n",
      "Using normal model\n",
      "LBFGS training took [11] iter.\n",
      "After training with LBFGS: \n",
      "SCORE =  0.8033898305084746\n",
      "\n",
      "WITH 28% MOST INFLUENTIALTRAINING POINTS:\n",
      "TOP N: 166\n",
      "Total number of parameters: 3\n",
      "Using normal model\n",
      "LBFGS training took [11] iter.\n",
      "After training with LBFGS: \n",
      "SCORE =  0.8033898305084746\n",
      "\n",
      "WITH 29% MOST INFLUENTIALTRAINING POINTS:\n",
      "TOP N: 172\n",
      "Total number of parameters: 3\n",
      "Using normal model\n",
      "LBFGS training took [11] iter.\n",
      "After training with LBFGS: \n",
      "SCORE =  0.8033898305084746\n",
      "\n",
      "WITH 30% MOST INFLUENTIALTRAINING POINTS:\n",
      "TOP N: 178\n",
      "Total number of parameters: 3\n",
      "Using normal model\n",
      "LBFGS training took [11] iter.\n",
      "After training with LBFGS: \n",
      "SCORE =  0.8033898305084746\n",
      "\n",
      "WITH 31% MOST INFLUENTIALTRAINING POINTS:\n",
      "TOP N: 184\n",
      "Total number of parameters: 3\n",
      "Using normal model\n",
      "LBFGS training took [11] iter.\n",
      "After training with LBFGS: \n",
      "SCORE =  0.8033898305084746\n",
      "\n",
      "WITH 32% MOST INFLUENTIALTRAINING POINTS:\n",
      "TOP N: 190\n",
      "Total number of parameters: 3\n",
      "Using normal model\n",
      "LBFGS training took [11] iter.\n",
      "After training with LBFGS: \n",
      "SCORE =  0.8033898305084746\n",
      "\n",
      "WITH 33% MOST INFLUENTIALTRAINING POINTS:\n",
      "TOP N: 196\n",
      "Total number of parameters: 3\n",
      "Using normal model\n",
      "LBFGS training took [11] iter.\n",
      "After training with LBFGS: \n",
      "SCORE =  0.8033898305084746\n",
      "\n",
      "WITH 34% MOST INFLUENTIALTRAINING POINTS:\n",
      "TOP N: 202\n",
      "Total number of parameters: 3\n",
      "Using normal model\n",
      "LBFGS training took [11] iter.\n",
      "After training with LBFGS: \n",
      "SCORE =  0.8033898305084746\n",
      "\n",
      "WITH 35% MOST INFLUENTIALTRAINING POINTS:\n",
      "TOP N: 208\n",
      "Total number of parameters: 3\n",
      "Using normal model\n",
      "LBFGS training took [11] iter.\n",
      "After training with LBFGS: \n",
      "SCORE =  0.8033898305084746\n",
      "\n",
      "WITH 36% MOST INFLUENTIALTRAINING POINTS:\n",
      "TOP N: 214\n",
      "Total number of parameters: 3\n",
      "Using normal model\n",
      "LBFGS training took [10] iter.\n",
      "After training with LBFGS: \n",
      "SCORE =  0.8033898305084746\n",
      "\n",
      "WITH 37% MOST INFLUENTIALTRAINING POINTS:\n",
      "TOP N: 220\n",
      "Total number of parameters: 3\n",
      "Using normal model\n",
      "LBFGS training took [10] iter.\n",
      "After training with LBFGS: \n",
      "SCORE =  0.8033898305084746\n",
      "\n",
      "WITH 38% MOST INFLUENTIALTRAINING POINTS:\n",
      "TOP N: 226\n",
      "Total number of parameters: 3\n",
      "Using normal model\n",
      "LBFGS training took [10] iter.\n",
      "After training with LBFGS: \n",
      "SCORE =  0.8033898305084746\n",
      "\n",
      "WITH 39% MOST INFLUENTIALTRAINING POINTS:\n",
      "TOP N: 232\n",
      "Total number of parameters: 3\n",
      "Using normal model\n",
      "LBFGS training took [10] iter.\n",
      "After training with LBFGS: \n",
      "SCORE =  0.8033898305084746\n",
      "\n",
      "WITH 40% MOST INFLUENTIALTRAINING POINTS:\n",
      "TOP N: 238\n",
      "Total number of parameters: 3\n",
      "Using normal model\n",
      "LBFGS training took [10] iter.\n",
      "After training with LBFGS: \n",
      "SCORE =  0.8033898305084746\n",
      "\n",
      "WITH 41% MOST INFLUENTIALTRAINING POINTS:\n",
      "TOP N: 244\n",
      "Total number of parameters: 3\n",
      "Using normal model\n",
      "LBFGS training took [10] iter.\n",
      "After training with LBFGS: \n",
      "SCORE =  0.8033898305084746\n",
      "\n",
      "WITH 42% MOST INFLUENTIALTRAINING POINTS:\n",
      "TOP N: 250\n",
      "Total number of parameters: 3\n",
      "Using normal model\n",
      "LBFGS training took [10] iter.\n",
      "After training with LBFGS: \n",
      "SCORE =  0.8033898305084746\n",
      "\n",
      "WITH 43% MOST INFLUENTIALTRAINING POINTS:\n",
      "TOP N: 256\n",
      "Total number of parameters: 3\n",
      "Using normal model\n",
      "LBFGS training took [10] iter.\n",
      "After training with LBFGS: \n",
      "SCORE =  0.8033898305084746\n",
      "\n",
      "WITH 44% MOST INFLUENTIALTRAINING POINTS:\n",
      "TOP N: 262\n",
      "Total number of parameters: 3\n",
      "Using normal model\n",
      "LBFGS training took [10] iter.\n",
      "After training with LBFGS: \n",
      "SCORE =  0.8033898305084746\n",
      "\n",
      "WITH 45% MOST INFLUENTIALTRAINING POINTS:\n",
      "TOP N: 268\n",
      "Total number of parameters: 3\n",
      "Using normal model\n",
      "LBFGS training took [10] iter.\n",
      "After training with LBFGS: \n",
      "SCORE =  0.8033898305084746\n",
      "\n",
      "WITH 46% MOST INFLUENTIALTRAINING POINTS:\n",
      "TOP N: 274\n",
      "Total number of parameters: 3\n",
      "Using normal model\n",
      "LBFGS training took [10] iter.\n",
      "After training with LBFGS: \n",
      "SCORE =  0.8033898305084746\n",
      "\n",
      "WITH 47% MOST INFLUENTIALTRAINING POINTS:\n",
      "TOP N: 280\n",
      "Total number of parameters: 3\n",
      "Using normal model\n",
      "LBFGS training took [10] iter.\n",
      "After training with LBFGS: \n",
      "SCORE =  0.8033898305084746\n",
      "\n",
      "WITH 48% MOST INFLUENTIALTRAINING POINTS:\n",
      "TOP N: 286\n",
      "Total number of parameters: 3\n",
      "Using normal model\n",
      "LBFGS training took [10] iter.\n",
      "After training with LBFGS: \n",
      "SCORE =  0.8033898305084746\n",
      "\n",
      "WITH 49% MOST INFLUENTIALTRAINING POINTS:\n",
      "TOP N: 292\n",
      "Total number of parameters: 3\n",
      "Using normal model\n",
      "LBFGS training took [10] iter.\n",
      "After training with LBFGS: \n",
      "SCORE =  0.8033898305084746\n",
      "\n",
      "WITH 50% MOST INFLUENTIALTRAINING POINTS:\n",
      "TOP N: 298\n",
      "Total number of parameters: 3\n",
      "Using normal model\n",
      "LBFGS training took [10] iter.\n",
      "After training with LBFGS: \n",
      "SCORE =  0.8033898305084746\n",
      "\n",
      "WITH 51% MOST INFLUENTIALTRAINING POINTS:\n",
      "TOP N: 303\n",
      "Total number of parameters: 3\n",
      "Using normal model\n",
      "LBFGS training took [10] iter.\n",
      "After training with LBFGS: \n",
      "SCORE =  0.8033898305084746\n",
      "\n",
      "WITH 52% MOST INFLUENTIALTRAINING POINTS:\n",
      "TOP N: 309\n",
      "Total number of parameters: 3\n",
      "Using normal model\n",
      "LBFGS training took [10] iter.\n",
      "After training with LBFGS: \n",
      "SCORE =  0.8033898305084746\n",
      "\n",
      "WITH 53% MOST INFLUENTIALTRAINING POINTS:\n",
      "TOP N: 315\n",
      "Total number of parameters: 3\n",
      "Using normal model\n",
      "LBFGS training took [10] iter.\n",
      "After training with LBFGS: \n",
      "SCORE =  0.8033898305084746\n",
      "\n",
      "WITH 54% MOST INFLUENTIALTRAINING POINTS:\n",
      "TOP N: 321\n",
      "Total number of parameters: 3\n",
      "Using normal model\n",
      "LBFGS training took [10] iter.\n",
      "After training with LBFGS: \n",
      "SCORE =  0.8033898305084746\n",
      "\n",
      "WITH 55% MOST INFLUENTIALTRAINING POINTS:\n",
      "TOP N: 327\n",
      "Total number of parameters: 3\n",
      "Using normal model\n",
      "LBFGS training took [10] iter.\n",
      "After training with LBFGS: \n",
      "SCORE =  0.8033898305084746\n",
      "\n",
      "WITH 56% MOST INFLUENTIALTRAINING POINTS:\n",
      "TOP N: 333\n",
      "Total number of parameters: 3\n",
      "Using normal model\n",
      "LBFGS training took [10] iter.\n",
      "After training with LBFGS: \n",
      "SCORE =  0.8033898305084746\n",
      "\n",
      "WITH 57% MOST INFLUENTIALTRAINING POINTS:\n",
      "TOP N: 339\n",
      "Total number of parameters: 3\n",
      "Using normal model\n",
      "LBFGS training took [10] iter.\n",
      "After training with LBFGS: \n",
      "SCORE =  0.8033898305084746\n",
      "\n",
      "WITH 58% MOST INFLUENTIALTRAINING POINTS:\n",
      "TOP N: 345\n",
      "Total number of parameters: 3\n",
      "Using normal model\n",
      "LBFGS training took [10] iter.\n",
      "After training with LBFGS: \n",
      "SCORE =  0.8033898305084746\n",
      "\n",
      "WITH 59% MOST INFLUENTIALTRAINING POINTS:\n",
      "TOP N: 351\n",
      "Total number of parameters: 3\n",
      "Using normal model\n",
      "LBFGS training took [10] iter.\n",
      "After training with LBFGS: \n",
      "SCORE =  0.8033898305084746\n",
      "\n",
      "WITH 60% MOST INFLUENTIALTRAINING POINTS:\n",
      "TOP N: 357\n",
      "Total number of parameters: 3\n",
      "Using normal model\n",
      "LBFGS training took [10] iter.\n",
      "After training with LBFGS: \n",
      "SCORE =  0.8033898305084746\n",
      "\n",
      "WITH 61% MOST INFLUENTIALTRAINING POINTS:\n",
      "TOP N: 363\n",
      "Total number of parameters: 3\n",
      "Using normal model\n",
      "LBFGS training took [10] iter.\n",
      "After training with LBFGS: \n",
      "SCORE =  0.8033898305084746\n",
      "\n",
      "WITH 62% MOST INFLUENTIALTRAINING POINTS:\n",
      "TOP N: 369\n",
      "Total number of parameters: 3\n",
      "Using normal model\n",
      "LBFGS training took [10] iter.\n",
      "After training with LBFGS: \n",
      "SCORE =  0.8033898305084746\n",
      "\n",
      "WITH 63% MOST INFLUENTIALTRAINING POINTS:\n",
      "TOP N: 375\n",
      "Total number of parameters: 3\n",
      "Using normal model\n",
      "LBFGS training took [10] iter.\n",
      "After training with LBFGS: \n",
      "SCORE =  0.8033898305084746\n",
      "\n",
      "WITH 64% MOST INFLUENTIALTRAINING POINTS:\n",
      "TOP N: 381\n",
      "Total number of parameters: 3\n",
      "Using normal model\n",
      "LBFGS training took [10] iter.\n",
      "After training with LBFGS: \n",
      "SCORE =  0.8033898305084746\n",
      "\n",
      "WITH 65% MOST INFLUENTIALTRAINING POINTS:\n",
      "TOP N: 387\n",
      "Total number of parameters: 3\n",
      "Using normal model\n",
      "LBFGS training took [10] iter.\n",
      "After training with LBFGS: \n",
      "SCORE =  0.8033898305084746\n",
      "\n",
      "WITH 66% MOST INFLUENTIALTRAINING POINTS:\n",
      "TOP N: 393\n",
      "Total number of parameters: 3\n",
      "Using normal model\n",
      "LBFGS training took [10] iter.\n",
      "After training with LBFGS: \n",
      "SCORE =  0.8033898305084746\n",
      "\n",
      "WITH 67% MOST INFLUENTIALTRAINING POINTS:\n",
      "TOP N: 399\n",
      "Total number of parameters: 3\n",
      "Using normal model\n",
      "LBFGS training took [10] iter.\n",
      "After training with LBFGS: \n",
      "SCORE =  0.8033898305084746\n",
      "\n",
      "WITH 68% MOST INFLUENTIALTRAINING POINTS:\n",
      "TOP N: 405\n",
      "Total number of parameters: 3\n",
      "Using normal model\n",
      "LBFGS training took [10] iter.\n",
      "After training with LBFGS: \n",
      "SCORE =  0.8033898305084746\n",
      "\n",
      "WITH 69% MOST INFLUENTIALTRAINING POINTS:\n",
      "TOP N: 411\n",
      "Total number of parameters: 3\n",
      "Using normal model\n",
      "LBFGS training took [10] iter.\n",
      "After training with LBFGS: \n",
      "SCORE =  0.8033898305084746\n",
      "\n",
      "WITH 70% MOST INFLUENTIALTRAINING POINTS:\n",
      "TOP N: 417\n",
      "Total number of parameters: 3\n",
      "Using normal model\n",
      "LBFGS training took [9] iter.\n",
      "After training with LBFGS: \n",
      "SCORE =  0.8033898305084746\n",
      "\n",
      "WITH 71% MOST INFLUENTIALTRAINING POINTS:\n",
      "TOP N: 423\n",
      "Total number of parameters: 3\n",
      "Using normal model\n",
      "LBFGS training took [9] iter.\n",
      "After training with LBFGS: \n",
      "SCORE =  0.8033898305084746\n",
      "\n",
      "WITH 72% MOST INFLUENTIALTRAINING POINTS:\n",
      "TOP N: 429\n",
      "Total number of parameters: 3\n",
      "Using normal model\n",
      "LBFGS training took [10] iter.\n",
      "After training with LBFGS: \n",
      "SCORE =  0.7796610169491526\n",
      "\n",
      "WITH 73% MOST INFLUENTIALTRAINING POINTS:\n",
      "TOP N: 435\n",
      "Total number of parameters: 3\n",
      "Using normal model\n",
      "LBFGS training took [10] iter.\n",
      "After training with LBFGS: \n",
      "SCORE =  0.7796610169491526\n",
      "\n",
      "WITH 74% MOST INFLUENTIALTRAINING POINTS:\n",
      "TOP N: 441\n",
      "Total number of parameters: 3\n",
      "Using normal model\n",
      "LBFGS training took [10] iter.\n",
      "After training with LBFGS: \n",
      "SCORE =  0.8033898305084746\n",
      "\n",
      "WITH 75% MOST INFLUENTIALTRAINING POINTS:\n",
      "TOP N: 447\n",
      "Total number of parameters: 3\n",
      "Using normal model\n",
      "LBFGS training took [11] iter.\n",
      "After training with LBFGS: \n",
      "SCORE =  0.8033898305084746\n",
      "\n",
      "WITH 76% MOST INFLUENTIALTRAINING POINTS:\n",
      "TOP N: 452\n",
      "Total number of parameters: 3\n",
      "Using normal model\n",
      "LBFGS training took [11] iter.\n",
      "After training with LBFGS: \n",
      "SCORE =  0.8033898305084746\n",
      "\n",
      "WITH 77% MOST INFLUENTIALTRAINING POINTS:\n",
      "TOP N: 458\n",
      "Total number of parameters: 3\n",
      "Using normal model\n",
      "LBFGS training took [9] iter.\n",
      "After training with LBFGS: \n",
      "SCORE =  0.8033898305084746\n",
      "\n",
      "WITH 78% MOST INFLUENTIALTRAINING POINTS:\n",
      "TOP N: 464\n",
      "Total number of parameters: 3\n",
      "Using normal model\n",
      "LBFGS training took [11] iter.\n",
      "After training with LBFGS: \n",
      "SCORE =  0.8033898305084746\n",
      "\n",
      "WITH 79% MOST INFLUENTIALTRAINING POINTS:\n",
      "TOP N: 470\n",
      "Total number of parameters: 3\n",
      "Using normal model\n",
      "LBFGS training took [11] iter.\n",
      "After training with LBFGS: \n",
      "SCORE =  0.8033898305084746\n",
      "\n",
      "WITH 80% MOST INFLUENTIALTRAINING POINTS:\n",
      "TOP N: 476\n",
      "Total number of parameters: 3\n",
      "Using normal model\n",
      "LBFGS training took [11] iter.\n",
      "After training with LBFGS: \n",
      "SCORE =  0.8033898305084746\n",
      "\n",
      "WITH 81% MOST INFLUENTIALTRAINING POINTS:\n",
      "TOP N: 482\n",
      "Total number of parameters: 3\n",
      "Using normal model\n",
      "LBFGS training took [10] iter.\n",
      "After training with LBFGS: \n",
      "SCORE =  0.8033898305084746\n",
      "\n",
      "WITH 82% MOST INFLUENTIALTRAINING POINTS:\n",
      "TOP N: 488\n",
      "Total number of parameters: 3\n",
      "Using normal model\n",
      "LBFGS training took [10] iter.\n",
      "After training with LBFGS: \n",
      "SCORE =  0.8033898305084746\n",
      "\n",
      "WITH 83% MOST INFLUENTIALTRAINING POINTS:\n",
      "TOP N: 494\n",
      "Total number of parameters: 3\n",
      "Using normal model\n",
      "LBFGS training took [11] iter.\n",
      "After training with LBFGS: \n",
      "SCORE =  0.8033898305084746\n",
      "\n",
      "WITH 84% MOST INFLUENTIALTRAINING POINTS:\n",
      "TOP N: 500\n",
      "Total number of parameters: 3\n",
      "Using normal model\n",
      "LBFGS training took [10] iter.\n",
      "After training with LBFGS: \n",
      "SCORE =  0.8033898305084746\n",
      "\n",
      "WITH 85% MOST INFLUENTIALTRAINING POINTS:\n",
      "TOP N: 506\n",
      "Total number of parameters: 3\n",
      "Using normal model\n",
      "LBFGS training took [10] iter.\n",
      "After training with LBFGS: \n",
      "SCORE =  0.8033898305084746\n",
      "\n",
      "WITH 86% MOST INFLUENTIALTRAINING POINTS:\n",
      "TOP N: 512\n",
      "Total number of parameters: 3\n",
      "Using normal model\n",
      "LBFGS training took [8] iter.\n",
      "After training with LBFGS: \n",
      "SCORE =  0.8033898305084746\n",
      "\n",
      "WITH 87% MOST INFLUENTIALTRAINING POINTS:\n",
      "TOP N: 518\n",
      "Total number of parameters: 3\n",
      "Using normal model\n",
      "LBFGS training took [9] iter.\n",
      "After training with LBFGS: \n",
      "SCORE =  0.8033898305084746\n",
      "\n",
      "WITH 88% MOST INFLUENTIALTRAINING POINTS:\n",
      "TOP N: 524\n",
      "Total number of parameters: 3\n",
      "Using normal model\n",
      "LBFGS training took [9] iter.\n",
      "After training with LBFGS: \n",
      "SCORE =  0.8033898305084746\n",
      "\n",
      "WITH 89% MOST INFLUENTIALTRAINING POINTS:\n",
      "TOP N: 530\n",
      "Total number of parameters: 3\n",
      "Using normal model\n",
      "LBFGS training took [9] iter.\n",
      "After training with LBFGS: \n",
      "SCORE =  0.7796610169491526\n",
      "\n",
      "WITH 90% MOST INFLUENTIALTRAINING POINTS:\n",
      "TOP N: 536\n",
      "Total number of parameters: 3\n",
      "Using normal model\n",
      "LBFGS training took [9] iter.\n",
      "After training with LBFGS: \n",
      "SCORE =  0.7796610169491526\n",
      "\n",
      "WITH 91% MOST INFLUENTIALTRAINING POINTS:\n",
      "TOP N: 542\n",
      "Total number of parameters: 3\n",
      "Using normal model\n",
      "LBFGS training took [8] iter.\n",
      "After training with LBFGS: \n",
      "SCORE =  0.7796610169491526\n",
      "\n",
      "WITH 92% MOST INFLUENTIALTRAINING POINTS:\n",
      "TOP N: 548\n",
      "Total number of parameters: 3\n",
      "Using normal model\n",
      "LBFGS training took [8] iter.\n",
      "After training with LBFGS: \n",
      "SCORE =  0.7796610169491526\n",
      "\n",
      "WITH 93% MOST INFLUENTIALTRAINING POINTS:\n",
      "TOP N: 554\n",
      "Total number of parameters: 3\n",
      "Using normal model\n",
      "LBFGS training took [8] iter.\n",
      "After training with LBFGS: \n",
      "SCORE =  0.7796610169491526\n",
      "\n",
      "WITH 94% MOST INFLUENTIALTRAINING POINTS:\n",
      "TOP N: 560\n",
      "Total number of parameters: 3\n",
      "Using normal model\n",
      "LBFGS training took [8] iter.\n",
      "After training with LBFGS: \n",
      "SCORE =  0.7796610169491526\n",
      "\n",
      "WITH 95% MOST INFLUENTIALTRAINING POINTS:\n",
      "TOP N: 566\n",
      "Total number of parameters: 3\n",
      "Using normal model\n",
      "LBFGS training took [8] iter.\n",
      "After training with LBFGS: \n",
      "SCORE =  0.7796610169491526\n",
      "\n",
      "WITH 96% MOST INFLUENTIALTRAINING POINTS:\n",
      "TOP N: 572\n",
      "Total number of parameters: 3\n",
      "Using normal model\n",
      "LBFGS training took [7] iter.\n",
      "After training with LBFGS: \n",
      "SCORE =  0.7796610169491526\n",
      "\n",
      "WITH 97% MOST INFLUENTIALTRAINING POINTS:\n",
      "TOP N: 578\n",
      "Total number of parameters: 3\n",
      "Using normal model\n",
      "LBFGS training took [7] iter.\n",
      "After training with LBFGS: \n",
      "SCORE =  0.7796610169491526\n",
      "\n",
      "WITH 98% MOST INFLUENTIALTRAINING POINTS:\n",
      "TOP N: 584\n",
      "Total number of parameters: 3\n",
      "Using normal model\n",
      "LBFGS training took [6] iter.\n",
      "After training with LBFGS: \n",
      "SCORE =  0.7796610169491526\n",
      "\n",
      "WITH 99% MOST INFLUENTIALTRAINING POINTS:\n",
      "TOP N: 590\n",
      "Total number of parameters: 3\n",
      "Using normal model\n",
      "LBFGS training took [6] iter.\n",
      "After training with LBFGS: \n",
      "SCORE =  0.7796610169491526\n",
      "\n",
      "WITH 100% MOST INFLUENTIALTRAINING POINTS:\n",
      "TOP N: 596\n",
      "Total number of parameters: 3\n",
      "Using normal model\n",
      "LBFGS training took [7] iter.\n",
      "After training with LBFGS: \n",
      "SCORE =  0.7796610169491526\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "([3,\n",
       "  4,\n",
       "  5,\n",
       "  6,\n",
       "  7,\n",
       "  8,\n",
       "  9,\n",
       "  10,\n",
       "  11,\n",
       "  12,\n",
       "  13,\n",
       "  14,\n",
       "  15,\n",
       "  16,\n",
       "  17,\n",
       "  18,\n",
       "  19,\n",
       "  20,\n",
       "  21,\n",
       "  22,\n",
       "  23,\n",
       "  24,\n",
       "  25,\n",
       "  26,\n",
       "  27,\n",
       "  28,\n",
       "  29,\n",
       "  30,\n",
       "  31,\n",
       "  32,\n",
       "  33,\n",
       "  34,\n",
       "  35,\n",
       "  36,\n",
       "  37,\n",
       "  38,\n",
       "  39,\n",
       "  40,\n",
       "  41,\n",
       "  42,\n",
       "  43,\n",
       "  44,\n",
       "  45,\n",
       "  46,\n",
       "  47,\n",
       "  48,\n",
       "  49,\n",
       "  50,\n",
       "  51,\n",
       "  52,\n",
       "  53,\n",
       "  54,\n",
       "  55,\n",
       "  56,\n",
       "  57,\n",
       "  58,\n",
       "  59,\n",
       "  60,\n",
       "  61,\n",
       "  62,\n",
       "  63,\n",
       "  64,\n",
       "  65,\n",
       "  66,\n",
       "  67,\n",
       "  68,\n",
       "  69,\n",
       "  70,\n",
       "  71,\n",
       "  72,\n",
       "  73,\n",
       "  74,\n",
       "  75,\n",
       "  76,\n",
       "  77,\n",
       "  78,\n",
       "  79,\n",
       "  80,\n",
       "  81,\n",
       "  82,\n",
       "  83,\n",
       "  84,\n",
       "  85,\n",
       "  86,\n",
       "  87,\n",
       "  88,\n",
       "  89,\n",
       "  90,\n",
       "  91,\n",
       "  92,\n",
       "  93,\n",
       "  94,\n",
       "  95,\n",
       "  96,\n",
       "  97,\n",
       "  98,\n",
       "  99,\n",
       "  100],\n",
       " [0.8033898305084746,\n",
       "  0.8033898305084746,\n",
       "  0.8033898305084746,\n",
       "  0.8033898305084746,\n",
       "  0.8033898305084746,\n",
       "  0.8033898305084746,\n",
       "  0.8033898305084746,\n",
       "  0.8033898305084746,\n",
       "  0.8033898305084746,\n",
       "  0.8033898305084746,\n",
       "  0.8033898305084746,\n",
       "  0.8033898305084746,\n",
       "  0.8033898305084746,\n",
       "  0.8033898305084746,\n",
       "  0.8033898305084746,\n",
       "  0.8033898305084746,\n",
       "  0.8033898305084746,\n",
       "  0.8033898305084746,\n",
       "  0.8033898305084746,\n",
       "  0.8033898305084746,\n",
       "  0.8033898305084746,\n",
       "  0.8033898305084746,\n",
       "  0.8033898305084746,\n",
       "  0.8033898305084746,\n",
       "  0.8033898305084746,\n",
       "  0.8033898305084746,\n",
       "  0.8033898305084746,\n",
       "  0.8033898305084746,\n",
       "  0.8033898305084746,\n",
       "  0.8033898305084746,\n",
       "  0.8033898305084746,\n",
       "  0.8033898305084746,\n",
       "  0.8033898305084746,\n",
       "  0.8033898305084746,\n",
       "  0.8033898305084746,\n",
       "  0.8033898305084746,\n",
       "  0.8033898305084746,\n",
       "  0.8033898305084746,\n",
       "  0.8033898305084746,\n",
       "  0.8033898305084746,\n",
       "  0.8033898305084746,\n",
       "  0.8033898305084746,\n",
       "  0.8033898305084746,\n",
       "  0.8033898305084746,\n",
       "  0.8033898305084746,\n",
       "  0.8033898305084746,\n",
       "  0.8033898305084746,\n",
       "  0.8033898305084746,\n",
       "  0.8033898305084746,\n",
       "  0.8033898305084746,\n",
       "  0.8033898305084746,\n",
       "  0.8033898305084746,\n",
       "  0.8033898305084746,\n",
       "  0.8033898305084746,\n",
       "  0.8033898305084746,\n",
       "  0.8033898305084746,\n",
       "  0.8033898305084746,\n",
       "  0.8033898305084746,\n",
       "  0.8033898305084746,\n",
       "  0.8033898305084746,\n",
       "  0.8033898305084746,\n",
       "  0.8033898305084746,\n",
       "  0.8033898305084746,\n",
       "  0.8033898305084746,\n",
       "  0.8033898305084746,\n",
       "  0.8033898305084746,\n",
       "  0.8033898305084746,\n",
       "  0.8033898305084746,\n",
       "  0.8033898305084746,\n",
       "  0.7796610169491526,\n",
       "  0.7796610169491526,\n",
       "  0.8033898305084746,\n",
       "  0.8033898305084746,\n",
       "  0.8033898305084746,\n",
       "  0.8033898305084746,\n",
       "  0.8033898305084746,\n",
       "  0.8033898305084746,\n",
       "  0.8033898305084746,\n",
       "  0.8033898305084746,\n",
       "  0.8033898305084746,\n",
       "  0.8033898305084746,\n",
       "  0.8033898305084746,\n",
       "  0.8033898305084746,\n",
       "  0.8033898305084746,\n",
       "  0.8033898305084746,\n",
       "  0.8033898305084746,\n",
       "  0.7796610169491526,\n",
       "  0.7796610169491526,\n",
       "  0.7796610169491526,\n",
       "  0.7796610169491526,\n",
       "  0.7796610169491526,\n",
       "  0.7796610169491526,\n",
       "  0.7796610169491526,\n",
       "  0.7796610169491526,\n",
       "  0.7796610169491526,\n",
       "  0.7796610169491526,\n",
       "  0.7796610169491526,\n",
       "  0.7796610169491526])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train = X_train[:, 0:3]\n",
    "X_test = X_test[:, 0:3]\n",
    "tf_model = get_model(X_train, X_test, y_train, y_test)\n",
    "top_train_influence = get_top_train_influence(tf_model)\n",
    "retrain_most_influential_train(top_train_influence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
