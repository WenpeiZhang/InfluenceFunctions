{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Find most important feature in a model predictions using influence functions\n",
    "\n",
    "Pang Wei Koh and Percy Lian show in their [\"Understanding Black-box Predictions via Influence Functions\"](https://arxiv.org/pdf/1703.04730.pdf) (ICML 2017) that influence functions can be used to approximate the learning effect of training data onto the predictions.\n",
    "An extension of this is the ability to approximate the effect of a given perturbation on a training point.\n",
    "\n",
    "The original authors demonstrate how this also enables engineering adversarial training attack. The attack consists in finding the most influential training input for a given test point prediction. Then approximating the perturbation that has the maximally negative learning effect on learning the test point.\n",
    "\n",
    "The author suggest the possibility of using influence functions to approximate the effect of perturbing the training point has a way to analyse the training effects of features.\n",
    "\n",
    "\n",
    "We use the original code from the offical authors repository for the model classes and various utilities. We use the Kaggle [Titanic dataset](https://www.kaggle.com/c/titanic/data) to experiment on the idea of using influence functions to approximate the effect of features on learning in a blackbox model.\n",
    "\n",
    "**Plan**\n",
    "\n",
    "1. Preprocess data\n",
    "2. Train model (logistic regression)\n",
    "3. Test model\n",
    "4. Engineer adversarial training training data to degrade the performance of the model.\n",
    "5. Analyse the noise added to the feature as a characterization of the influence of feature in learning for the model.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "3. For each correctly labelled test point:\n",
    "       Find most their influential training points\n",
    "       Approximate the perturbation effect on each point\n",
    "       \n",
    "+ For each feature:\n",
    "\tFor each train point:\n",
    "\t    Get influence on loss of perturbation of given training point on given feature\n",
    "+ Get stats:\n",
    "\t+ Average by feature of their influence on z-test\n",
    "\t+(other stats)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparation of the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PassengerId</th>\n",
       "      <th>Survived</th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Name</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Age</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Ticket</th>\n",
       "      <th>Fare</th>\n",
       "      <th>Cabin</th>\n",
       "      <th>Embarked</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Braund, Mr. Owen Harris</td>\n",
       "      <td>male</td>\n",
       "      <td>22.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>A/5 21171</td>\n",
       "      <td>7.2500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Cumings, Mrs. John Bradley (Florence Briggs Th...</td>\n",
       "      <td>female</td>\n",
       "      <td>38.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>PC 17599</td>\n",
       "      <td>71.2833</td>\n",
       "      <td>C85</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>Heikkinen, Miss. Laina</td>\n",
       "      <td>female</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>STON/O2. 3101282</td>\n",
       "      <td>7.9250</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Futrelle, Mrs. Jacques Heath (Lily May Peel)</td>\n",
       "      <td>female</td>\n",
       "      <td>35.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>113803</td>\n",
       "      <td>53.1000</td>\n",
       "      <td>C123</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Allen, Mr. William Henry</td>\n",
       "      <td>male</td>\n",
       "      <td>35.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>373450</td>\n",
       "      <td>8.0500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   PassengerId  Survived  Pclass  \\\n",
       "0            1         0       3   \n",
       "1            2         1       1   \n",
       "2            3         1       3   \n",
       "3            4         1       1   \n",
       "4            5         0       3   \n",
       "\n",
       "                                                Name     Sex   Age  SibSp  \\\n",
       "0                            Braund, Mr. Owen Harris    male  22.0      1   \n",
       "1  Cumings, Mrs. John Bradley (Florence Briggs Th...  female  38.0      1   \n",
       "2                             Heikkinen, Miss. Laina  female  26.0      0   \n",
       "3       Futrelle, Mrs. Jacques Heath (Lily May Peel)  female  35.0      1   \n",
       "4                           Allen, Mr. William Henry    male  35.0      0   \n",
       "\n",
       "   Parch            Ticket     Fare Cabin Embarked  \n",
       "0      0         A/5 21171   7.2500   NaN        S  \n",
       "1      0          PC 17599  71.2833   C85        C  \n",
       "2      0  STON/O2. 3101282   7.9250   NaN        S  \n",
       "3      0            113803  53.1000  C123        S  \n",
       "4      0            373450   8.0500   NaN        S  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "fpath_titanic = \"/home/eolus/Desktop/Dauphine/datamining/projets/blackBox/data/train.csv\"\n",
    "train_df = pd.read_csv(fpath_titanic)\n",
    "\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def extract_prefix(name):\n",
    "    import re\n",
    "    try:\n",
    "        return re.search('(Mr\\.)|(Mrs\\.)|(Miss\\.)', name).group()\n",
    "    except:\n",
    "        return \"\"\n",
    "\n",
    "\n",
    "train_df['Prefix'] = train_df.Name.apply(extract_prefix)\n",
    "\n",
    "for cat_col in ['Sex', 'Embarked', 'Prefix' ]:\n",
    "    train_df[cat_col] = pd.factorize(train_df[cat_col])[0]\n",
    "    \n",
    "train_df['Age'].fillna(train_df.Age.mean(), inplace=True)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "features = ['Prefix', 'Pclass', 'Sex', 'Age', 'SibSp', 'Parch', 'Fare', 'Embarked']\n",
    "\n",
    "X = np.array(train_df[features])\n",
    "y = np.array((train_df.Survived > 0).astype('int32'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Scale\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(X)\n",
    "\n",
    "# Need to train/test split\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Create dataset objects\n",
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "\n",
    "#from influence indataset as dataset\n",
    "from influence.dataset import DataSet\n",
    "import numpy as np\n",
    "lr_train = DataSet(X_train, np.array(y_train, dtype=int))\n",
    "lr_test = DataSet(X_test, np.array(y_test, dtype=int))\n",
    "lr_validation = None\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.contrib.learn.python.learn.datasets import base\n",
    "lr_data_sets = base.Datasets(train=lr_train, validation=lr_validation, test=lr_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train model of reference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/eolus/.local/lib/python3.6/site-packages/tensorflow/python/ops/nn_grad.py:249: calling reduce_sum (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "keep_dims is deprecated, use keepdims instead\n",
      "Total number of parameters: 8\n",
      "Using normal model\n",
      "LBFGS training took [10] iter.\n",
      "After training with LBFGS: \n"
     ]
    }
   ],
   "source": [
    "from influence.binaryLogisticRegressionWithLBFGS import BinaryLogisticRegressionWithLBFGS\n",
    "\n",
    "num_classes = 2\n",
    "input_dim = len(features)\n",
    "\n",
    "weight_decay = 0.01\n",
    "batch_size = 100\n",
    "initial_learning_rate = 0.001 \n",
    "keep_probs = None\n",
    "decay_epochs = [1000, 10000]\n",
    "max_lbfgs_iter = 1000\n",
    "\n",
    "tf.reset_default_graph()\n",
    "\n",
    "tf_model = BinaryLogisticRegressionWithLBFGS(\n",
    "    input_dim=input_dim,\n",
    "    weight_decay=weight_decay,\n",
    "    max_lbfgs_iter=max_lbfgs_iter,\n",
    "    num_classes=num_classes, \n",
    "    batch_size=batch_size,\n",
    "    data_sets=lr_data_sets,\n",
    "    initial_learning_rate=initial_learning_rate,\n",
    "    keep_probs=keep_probs,\n",
    "    decay_epochs=decay_epochs,\n",
    "    mini_batch=False,\n",
    "    train_dir='tmp',\n",
    "    log_dir='tmp',\n",
    "    model_name='titanic')\n",
    "\n",
    "tf_model.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "true_positives: 100\n",
      "true_negatives: 137\n",
      "false_positives 38\n",
      "false_negatives 20\n"
     ]
    }
   ],
   "source": [
    "# Retrieve test predictions and reference labels\n",
    "preds_p = tf_model.get_preds().tolist()\n",
    "preds = [1 if el[0] < 0.5 else 0 for el in preds_p]\n",
    "ref = tf_model.data_sets.test.labels\n",
    "\n",
    "# True/False - Positives/Negatives    \n",
    "true_pos = [(i, p) for i, p in enumerate(preds_p) if p[0] < p[1] and ref[i] == 1]\n",
    "true_neg = [(i, p) for i, p in enumerate(preds_p) if p[0] > p[1] and ref[i] == 0]\n",
    "false_pos = [(i, p) for i, p in enumerate(preds_p) if p[0] < p[1] and ref[i] == 0]\n",
    "false_neg = [(i, p) for i, p in enumerate(preds_p) if p[0] > p[1] and ref[i] == 1]\n",
    "\n",
    "# Confusion matrix data\n",
    "print(\"true_positives:\", len(true_pos))\n",
    "print(\"true_negatives:\", len(true_neg))\n",
    "print(\"false_positives\", len(false_pos))\n",
    "print(\"false_negatives\", len(false_neg))\n",
    "\n",
    "# Sort true_positives and true_negatives by how confident the model is\n",
    "true_pos_top = sorted(true_pos, key=lambda x : x[1][0], reverse=False)\n",
    "true_neg_top = sorted(true_neg, key=lambda x : x[1][0], reverse=True)\n",
    "\n",
    "# Sample down (top 10)\n",
    "true_pos_top = true_pos_top[:10]\n",
    "true_neg_top = true_neg_top[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find most influential train points\n",
    "\n",
    "For each true positive / true negative prediction for which the model predicted with high confidence, we approximate which train points are most responsible for the prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_top_train_influence(idx, n_train_points=5):\n",
    "    \"\"\"\n",
    "    Approximate most influential train points for a test point\n",
    "    idx : index of test point\n",
    "    \"\"\"\n",
    "    num_train = len(tf_model.data_sets.train.labels)\n",
    "    influences = tf_model.get_influence_on_test_loss(\n",
    "        [idx], \n",
    "        np.arange(len(tf_model.data_sets.train.labels)),\n",
    "        force_refresh=True) * num_train\n",
    "    influences_sorted = sorted(enumerate(influences),\n",
    "                               key=lambda x:x[1],\n",
    "                               reverse=True)\n",
    "    influences_sorted = influences_sorted[:n_train_points]\n",
    "    return influences_sorted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: -0.026063\n",
      "         Iterations: 4\n",
      "         Function evaluations: 89\n",
      "         Gradient evaluations: 82\n",
      "         Hessian evaluations: 19\n",
      "Optimization terminated successfully.\n",
      "         Current function value: -0.030229\n",
      "         Iterations: 6\n",
      "         Function evaluations: 7\n",
      "         Gradient evaluations: 12\n",
      "         Hessian evaluations: 21\n",
      "Optimization terminated successfully.\n",
      "         Current function value: -0.064258\n",
      "         Iterations: 6\n",
      "         Function evaluations: 7\n",
      "         Gradient evaluations: 12\n",
      "         Hessian evaluations: 22\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: -0.030219\n",
      "         Iterations: 5\n",
      "         Function evaluations: 134\n",
      "         Gradient evaluations: 126\n",
      "         Hessian evaluations: 26\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: -0.070613\n",
      "         Iterations: 5\n",
      "         Function evaluations: 74\n",
      "         Gradient evaluations: 69\n",
      "         Hessian evaluations: 28\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: -0.024056\n",
      "         Iterations: 4\n",
      "         Function evaluations: 87\n",
      "         Gradient evaluations: 79\n",
      "         Hessian evaluations: 19\n",
      "Optimization terminated successfully.\n",
      "         Current function value: -0.098777\n",
      "         Iterations: 6\n",
      "         Function evaluations: 7\n",
      "         Gradient evaluations: 12\n",
      "         Hessian evaluations: 20\n",
      "Optimization terminated successfully.\n",
      "         Current function value: -0.029434\n",
      "         Iterations: 6\n",
      "         Function evaluations: 7\n",
      "         Gradient evaluations: 12\n",
      "         Hessian evaluations: 18\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: -0.052283\n",
      "         Iterations: 4\n",
      "         Function evaluations: 80\n",
      "         Gradient evaluations: 72\n",
      "         Hessian evaluations: 16\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: -0.033581\n",
      "         Iterations: 5\n",
      "         Function evaluations: 157\n",
      "         Gradient evaluations: 147\n",
      "         Hessian evaluations: 26\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: -0.219252\n",
      "         Iterations: 7\n",
      "         Function evaluations: 9\n",
      "         Gradient evaluations: 15\n",
      "         Hessian evaluations: 25\n",
      "Optimization terminated successfully.\n",
      "         Current function value: -0.474698\n",
      "         Iterations: 6\n",
      "         Function evaluations: 7\n",
      "         Gradient evaluations: 12\n",
      "         Hessian evaluations: 22\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: -0.190590\n",
      "         Iterations: 7\n",
      "         Function evaluations: 19\n",
      "         Gradient evaluations: 25\n",
      "         Hessian evaluations: 23\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: -0.358243\n",
      "         Iterations: 5\n",
      "         Function evaluations: 69\n",
      "         Gradient evaluations: 62\n",
      "         Hessian evaluations: 23\n",
      "Optimization terminated successfully.\n",
      "         Current function value: -0.291692\n",
      "         Iterations: 7\n",
      "         Function evaluations: 8\n",
      "         Gradient evaluations: 14\n",
      "         Hessian evaluations: 21\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: -0.162535\n",
      "         Iterations: 5\n",
      "         Function evaluations: 81\n",
      "         Gradient evaluations: 74\n",
      "         Hessian evaluations: 19\n",
      "Optimization terminated successfully.\n",
      "         Current function value: -0.166461\n",
      "         Iterations: 7\n",
      "         Function evaluations: 8\n",
      "         Gradient evaluations: 14\n",
      "         Hessian evaluations: 19\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: -0.265753\n",
      "         Iterations: 5\n",
      "         Function evaluations: 148\n",
      "         Gradient evaluations: 139\n",
      "         Hessian evaluations: 26\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: -0.311592\n",
      "         Iterations: 5\n",
      "         Function evaluations: 78\n",
      "         Gradient evaluations: 71\n",
      "         Hessian evaluations: 23\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: -0.823891\n",
      "         Iterations: 5\n",
      "         Function evaluations: 76\n",
      "         Gradient evaluations: 70\n",
      "         Hessian evaluations: 23\n"
     ]
    }
   ],
   "source": [
    "# Get test points indices\n",
    "true_pos_top_idx = [top_pos[0] for top_pos in true_pos_top]\n",
    "true_neg_top_idx = [top_neg[0] for top_neg in true_neg_top]\n",
    "\n",
    "# Approximate most influential train points for each test point\n",
    "influence_train_true_pos = [get_top_train_influence(idx) for idx in true_pos_top_idx]\n",
    "influence_train_true_neg = [get_top_train_influence(idx) for idx in true_neg_top_idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find most important features for each prediction using the influence perturbation function\n",
    "\n",
    "For each influential train point, we get the gradient of influence wrt to input to estimate the perturbation that optimally effect learning performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_top_pert_influence(train_indices, test_idx):\n",
    "    \"\"\"\n",
    "    Approximate most grad of influence wrt training points in order to\n",
    "    find most important feature.\n",
    "    \"\"\"\n",
    "    influences_grad = tf_model.get_grad_of_influence_wrt_input(\n",
    "        train_indices, [test_idx], force_refresh=False)\n",
    "    return influences_grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "true_pos_influences_grad = [] \n",
    "for test_idx, train_top in zip(true_pos_top_idx, influence_train_true_pos):\n",
    "    train_indices = [r[0] for r in train_top]\n",
    "    top_pert_influence = get_top_pert_influence(train_indices, test_idx)\n",
    "    true_pos_influences_grad.append(top_pert_influence)\n",
    "\n",
    "true_neg_influences_grad = [] \n",
    "for test_idx, train_top in zip(true_neg_top_idx, influence_train_true_neg):\n",
    "    train_indices = [r[0] for r in train_top]\n",
    "    top_pert_influence = get_top_pert_influence(train_indices, test_idx)\n",
    "    true_neg_influences_grad.append(top_pert_influence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis\n",
    "\n",
    "We now observe the gradient of the influence wrt to the input features.\n",
    "The influence of the perturbations of the training points on the loss tells us about which features are important for the model predictions.\n",
    "\n",
    "We retrieve the perturbations of the inputs that maximally effect the loss on the test points.\n",
    "We analyse the perturbation to understand which feature drive the model predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def feature_stats(features, influences_grad):\n",
    "    grad_features_values = { f : [] for f in features }\n",
    "    for test_data in influences_grad:\n",
    "        for train_data in test_data:\n",
    "            for i, grad in enumerate(train_data):\n",
    "                grad_features_values[features[i]].append(grad)\n",
    "\n",
    "    import numpy as np          \n",
    "    tups = [\n",
    "        (f_name, np.var(grad_values), np.average(grad_values))\n",
    "        for f_name, grad_values in grad_features_values.items()\n",
    "        ]\n",
    "\n",
    "    tups = sorted(tups, key=lambda x: abs(x[2]), reverse=True)\n",
    "\n",
    "    for feature, var, avg in tups:\n",
    "        print(feature, \"\\n\\tAVG:\", avg, \"\\n\\tVAR:\", var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRUE POSITIVES INFLUENTIAL FEATURES\n",
      "Sex \n",
      "\tAVG: 0.08644320771098137 \n",
      "\tVAR: 0.018102367484395244\n",
      "Fare \n",
      "\tAVG: 0.04377822086215019 \n",
      "\tVAR: 0.26602818861633526\n",
      "SibSp \n",
      "\tAVG: -0.038868099302053455 \n",
      "\tVAR: 0.006965144332448826\n",
      "Prefix \n",
      "\tAVG: 0.026565461233258248 \n",
      "\tVAR: 0.004746210058479657\n",
      "Parch \n",
      "\tAVG: 0.010333593487739563 \n",
      "\tVAR: 0.006717591137467977\n",
      "Pclass \n",
      "\tAVG: -0.008629011064767838 \n",
      "\tVAR: 0.04301512469167615\n",
      "Embarked \n",
      "\tAVG: -0.006246002819389105 \n",
      "\tVAR: 0.009715120401173111\n",
      "Age \n",
      "\tAVG: 0.0061956533044576646 \n",
      "\tVAR: 0.004426014880525523\n",
      "\n",
      "\n",
      "TRUE NEGATIVE INFLUENTIAL FEATURES\n",
      "Sex \n",
      "\tAVG: 1.0758050793409348 \n",
      "\tVAR: 1.0155880859589603\n",
      "Pclass \n",
      "\tAVG: -0.5493700152635574 \n",
      "\tVAR: 0.5984204174622609\n",
      "SibSp \n",
      "\tAVG: -0.37145029664039614 \n",
      "\tVAR: 0.45866864267915786\n",
      "Prefix \n",
      "\tAVG: 0.3709016835689545 \n",
      "\tVAR: 0.1812412449620621\n",
      "Fare \n",
      "\tAVG: 0.3173151677846909 \n",
      "\tVAR: 0.10644424084983299\n",
      "Parch \n",
      "\tAVG: 0.1713819009065628 \n",
      "\tVAR: 0.24954088902331611\n",
      "Age \n",
      "\tAVG: -0.12773430049419404 \n",
      "\tVAR: 0.38377715537157897\n",
      "Embarked \n",
      "\tAVG: 0.09917297333478928 \n",
      "\tVAR: 0.05454902719622378\n"
     ]
    }
   ],
   "source": [
    "print(\"TRUE POSITIVES INFLUENTIAL FEATURES\")\n",
    "feature_stats(features, true_pos_influences_grad)\n",
    "\n",
    "print(\"\\n\\nTRUE NEGATIVE INFLUENTIAL FEATURES\")\n",
    "feature_stats(features, true_neg_influences_grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Benchmark feature importance\n",
    "\n",
    "+ Mutual information\n",
    "+ Recursive feature elimination (using logit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mutual information scores:\n",
      "----------------------------\n",
      "0.165 Sex\n",
      "0.156 Prefix\n",
      "0.143 Fare\n",
      "0.074 Pclass\n",
      "0.032 Age\n",
      "0.022 Embarked\n",
      "0.014 SibSp\n",
      "0.0 Parch\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_selection import mutual_info_classif\n",
    "mi_scores = sorted(zip(features, mutual_info_classif(X, y)), key=lambda x : x[1], reverse=True)\n",
    "print(\"Mutual information scores:\")\n",
    "print(\"----------------------------\")\n",
    "for mi in mi_scores:\n",
    "    print(\"{} {}\".format(round(mi[1], 3), mi[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logit regression coefficients, test:0.82\n",
      "----------------------------------------------\n",
      "1.0 Sex\n",
      "0.72 Pclass\n",
      "0.39 Prefix\n",
      "0.38 SibSp\n",
      "0.29 Age\n",
      "0.19 Fare\n",
      "0.16 Embarked\n",
      "0.1 Parch\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_selection import RFECV\n",
    "\n",
    "estimator = LogisticRegression()\n",
    "selector = RFECV(estimator, step=3, cv=3)\n",
    "selector = selector.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "features_importance = zip(features, selector.estimator_.coef_[0])\n",
    "sorted_logit_coefs = sorted(features_importance, key=lambda x : abs(x[1]), reverse=True)\n",
    "print(\"Logit regression coefficients, test:{}\".format(round(selector.score(X_test, y_test),  2)))\n",
    "print(\"----------------------------------------------\")\n",
    "for el in sorted_logit_coefs:\n",
    "    print(round(abs(el[1]), 2), el[0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
