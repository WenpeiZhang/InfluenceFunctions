{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Find most important feature in a model predictions using influence functions\n",
    "\n",
    "Pang Wei Koh and Percy Lian show in their [\"Understanding Black-box Predictions via Influence Functions\"](https://arxiv.org/pdf/1703.04730.pdf) (ICML 2017) that influence functions can be used to approximate the learning effect of training data onto the predictions.\n",
    "An extension of this is the ability to approximate the effect of a given perturbation on a training point.\n",
    "\n",
    "The original authors demonstrate how this also enables engineering adversarial training attack. The attack consists in finding the most influential training input for a given test point prediction. Then approximating the perturbation that has the maximally negative learning effect on learning the test point.\n",
    "\n",
    "The author suggest the possibility of using influence functions to approximate the effect of perturbing the training point has a way to analyse the training effects of features.\n",
    "\n",
    "\n",
    "We use the original code from the offical authors repository for the model classes and various utilities. We use the Kaggle [Titanic dataset](https://www.kaggle.com/c/titanic/data) to experiment on the idea of using influence functions to approximate the effect of features on learning in a blackbox model.\n",
    "\n",
    "**Plan**\n",
    "\n",
    "1. Preprocess data\n",
    "2. Train model (logistic regression)\n",
    "3. Test model\n",
    "4. Engineer adversarial training training data to degrade the performance of the model.\n",
    "5. Analyse the noise added to the feature as a characterization of the influence of feature in learning for the model.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "3. For each correctly labelled test point:\n",
    "       Find most their influential training points\n",
    "       Approximate the perturbation effect on each point\n",
    "       \n",
    "+ For each feature:\n",
    "\tFor each train point:\n",
    "\t    Get influence on loss of perturbation of given training point on given feature\n",
    "+ Get stats:\n",
    "\t+ Average by feature of their influence on z-test\n",
    "\t+(other stats)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparation of the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PassengerId</th>\n",
       "      <th>Survived</th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Name</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Age</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Ticket</th>\n",
       "      <th>Fare</th>\n",
       "      <th>Cabin</th>\n",
       "      <th>Embarked</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Braund, Mr. Owen Harris</td>\n",
       "      <td>male</td>\n",
       "      <td>22.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>A/5 21171</td>\n",
       "      <td>7.2500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Cumings, Mrs. John Bradley (Florence Briggs Th...</td>\n",
       "      <td>female</td>\n",
       "      <td>38.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>PC 17599</td>\n",
       "      <td>71.2833</td>\n",
       "      <td>C85</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>Heikkinen, Miss. Laina</td>\n",
       "      <td>female</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>STON/O2. 3101282</td>\n",
       "      <td>7.9250</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Futrelle, Mrs. Jacques Heath (Lily May Peel)</td>\n",
       "      <td>female</td>\n",
       "      <td>35.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>113803</td>\n",
       "      <td>53.1000</td>\n",
       "      <td>C123</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Allen, Mr. William Henry</td>\n",
       "      <td>male</td>\n",
       "      <td>35.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>373450</td>\n",
       "      <td>8.0500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   PassengerId  Survived  Pclass  \\\n",
       "0            1         0       3   \n",
       "1            2         1       1   \n",
       "2            3         1       3   \n",
       "3            4         1       1   \n",
       "4            5         0       3   \n",
       "\n",
       "                                                Name     Sex   Age  SibSp  \\\n",
       "0                            Braund, Mr. Owen Harris    male  22.0      1   \n",
       "1  Cumings, Mrs. John Bradley (Florence Briggs Th...  female  38.0      1   \n",
       "2                             Heikkinen, Miss. Laina  female  26.0      0   \n",
       "3       Futrelle, Mrs. Jacques Heath (Lily May Peel)  female  35.0      1   \n",
       "4                           Allen, Mr. William Henry    male  35.0      0   \n",
       "\n",
       "   Parch            Ticket     Fare Cabin Embarked  \n",
       "0      0         A/5 21171   7.2500   NaN        S  \n",
       "1      0          PC 17599  71.2833   C85        C  \n",
       "2      0  STON/O2. 3101282   7.9250   NaN        S  \n",
       "3      0            113803  53.1000  C123        S  \n",
       "4      0            373450   8.0500   NaN        S  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "fpath_titanic = \"/home/eolus/Desktop/Dauphine/datamining/projets/blackBox/data/train.csv\"\n",
    "train_df = pd.read_csv(fpath_titanic)\n",
    "\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def extract_prefix(name):\n",
    "    import re\n",
    "    try:\n",
    "        return re.search('(Mr\\.)|(Mrs\\.)|(Miss\\.)', name).group()\n",
    "    except:\n",
    "        return \"\"\n",
    "\n",
    "\n",
    "train_df['Prefix'] = train_df.Name.apply(extract_prefix)\n",
    "\n",
    "for cat_col in ['Sex', 'Embarked', 'Prefix' ]:\n",
    "    train_df[cat_col] = pd.factorize(train_df[cat_col])[0]\n",
    "    \n",
    "train_df['Age'].fillna(train_df.Age.mean(), inplace=True)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "features = ['Prefix', 'Pclass', 'Sex', 'Age', 'SibSp', 'Parch', 'Fare', 'Embarked']\n",
    "\n",
    "X = np.array(train_df[features])\n",
    "y = np.array((train_df.Survived > 0).astype('int32'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Scale\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(X)\n",
    "\n",
    "# Need to train/test split\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Create dataset objects\n",
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "\n",
    "#from influence indataset as dataset\n",
    "from influence.dataset import DataSet\n",
    "import numpy as np\n",
    "lr_train = DataSet(X_train, np.array(y_train, dtype=int))\n",
    "lr_test = DataSet(X_test, np.array(y_test, dtype=int))\n",
    "lr_validation = None\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.contrib.learn.python.learn.datasets import base\n",
    "lr_data_sets = base.Datasets(train=lr_train, validation=lr_validation, test=lr_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train model of reference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/eolus/.local/lib/python3.6/site-packages/tensorflow/python/ops/nn_grad.py:249: calling reduce_sum (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "keep_dims is deprecated, use keepdims instead\n",
      "Total number of parameters: 8\n",
      "Using normal model\n",
      "LBFGS training took [10] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.49039847\n",
      "Train loss (w/o reg) on all data: 0.48153788\n",
      "Test loss (w/o reg) on all data: 0.43405235\n",
      "Train acc on all data:  0.7701342281879194\n",
      "Test acc on all data:   0.8033898305084746\n",
      "Norm of the mean of gradients: 4.7753606e-06\n",
      "Norm of the params: 1.33121\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/eolus/.local/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:97: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    }
   ],
   "source": [
    "from influence.binaryLogisticRegressionWithLBFGS import BinaryLogisticRegressionWithLBFGS\n",
    "\n",
    "num_classes = 2\n",
    "input_dim = len(features)\n",
    "\n",
    "weight_decay = 0.01\n",
    "batch_size = 100\n",
    "initial_learning_rate = 0.001 \n",
    "keep_probs = None\n",
    "decay_epochs = [1000, 10000]\n",
    "max_lbfgs_iter = 1000\n",
    "\n",
    "tf.reset_default_graph()\n",
    "\n",
    "tf_model = BinaryLogisticRegressionWithLBFGS(\n",
    "    input_dim=input_dim,\n",
    "    weight_decay=weight_decay,\n",
    "    max_lbfgs_iter=max_lbfgs_iter,\n",
    "    num_classes=num_classes, \n",
    "    batch_size=batch_size,\n",
    "    data_sets=lr_data_sets,\n",
    "    initial_learning_rate=initial_learning_rate,\n",
    "    keep_probs=keep_probs,\n",
    "    decay_epochs=decay_epochs,\n",
    "    mini_batch=False,\n",
    "    train_dir='tmp',\n",
    "    log_dir='tmp',\n",
    "    model_name='titanic')\n",
    "\n",
    "tf_model.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'DataSet' object has no attribute 'pred'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-105-8e41e36a09bc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#IM trying to find correctly labeled records\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtf_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata_sets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpred\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'DataSet' object has no attribute 'pred'"
     ]
    }
   ],
   "source": [
    "#IM trying to find correctly labeled records\n",
    "tf_model.data_sets.train.pred == y_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Select 10 most influential train point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Norm of test gradient: 0.39605683\n",
      "Function value: -0.3568248748779297\n",
      "Split function value: 0.3602283000946045, -0.7170532\n",
      "Predicted loss diff on train_idx 5: 0.0003191828977741651\n",
      "Function value: -0.37047380208969116\n",
      "Split function value: 0.3354359269142151, -0.7059097\n",
      "Predicted loss diff on train_idx 5: 9.387484032835736e-05\n",
      "Function value: -0.3753376007080078\n",
      "Split function value: 0.3806424140930176, -0.75598\n",
      "Predicted loss diff on train_idx 5: 0.00011002546138811431\n",
      "Function value: -0.3759527802467346\n",
      "Split function value: 0.374022901058197, -0.7499757\n",
      "Predicted loss diff on train_idx 5: 0.00010968234715045698\n",
      "Function value: -0.3759610950946808\n",
      "Split function value: 0.3758927881717682, -0.7518539\n",
      "Predicted loss diff on train_idx 5: 0.00011302833289108021\n",
      "Function value: -0.3759611248970032\n",
      "Split function value: 0.3759269118309021, -0.75188804\n",
      "Predicted loss diff on train_idx 5: 0.00011294900170908679\n",
      "Function value: -0.37596118450164795\n",
      "Split function value: 0.37594401836395264, -0.7519052\n",
      "Predicted loss diff on train_idx 5: 0.0001129092986151676\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: -0.375961\n",
      "         Iterations: 7\n",
      "         Function evaluations: 205\n",
      "         Gradient evaluations: 199\n",
      "         Hessian evaluations: 29\n",
      "Saved inverse HVP to tmp/titanic-cg-normal_loss-test-[30].npz\n",
      "Inverse HVP took 0.30474019050598145 sec\n",
      "Multiplying by 596 train examples took 0.1531507968902588 sec\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(596,)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_idx = 30\n",
    "\n",
    "num_train = len(tf_model.data_sets.train.labels)\n",
    "\n",
    "influences = tf_model.get_influence_on_test_loss(\n",
    "    [test_idx], \n",
    "    np.arange(len(tf_model.data_sets.train.labels)),\n",
    "    force_refresh=True) * num_train\n",
    "\n",
    "influences.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Do we want look at absolute value or only positive influence???\n",
    "top_influence_ix = [el[0] for el in sorted(list(enumerate(influences.tolist())), key=lambda x : x[1], reverse=True)[:30]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Influence perturbation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Check poison_with_influence_proj_gradient_step (GenericNeuralNet)\n",
    "# Check self.gradient_wrt_input_op (gradient gives direction of feature perturbation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Norm of test gradient: 0.39605683\n",
      "Loaded inverse HVP from tmp/titanic-cg-normal_loss-test-[30].npz\n",
      "Inverse HVP took 0.0016243457794189453 sec\n"
     ]
    }
   ],
   "source": [
    "test_idx = 30\n",
    "\n",
    "num_train = len(tf_model.data_sets.train.labels)\n",
    "\n",
    "influences_grad = tf_model.get_grad_of_influence_wrt_input(\n",
    "    np.arange(num_train),\n",
    "    [test_idx], \n",
    "    force_refresh=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "influence_grad_top_features = influences_grad[top_influence_ix, :]\n",
    "\n",
    "avg_inf_pert = [np.mean(col) for col in influence_grad_top_features.T]\n",
    "features_avg_inf = zip(features, avg_inf_pert)\n",
    "\n",
    "sorted_logit_influence = sorted(features_avg_inf, key=lambda x : abs(x[1]), reverse=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Benchmark feature importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8203389830508474"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_selection import RFECV\n",
    "\n",
    "estimator = LogisticRegression()\n",
    "selector = RFECV(estimator, step=3, cv=3)\n",
    "selector = selector.fit(X_train, y_train)\n",
    "selector.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.38991178, -0.71842588,  1.00059846, -0.29151786, -0.37872978,\n",
       "       -0.0958028 ,  0.1934243 ,  0.16377468])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "selector.estimator_.coef_[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Logistic Regression Coeffs\n",
      "============================\n",
      "('Sex', 1.0005984599032838)\n",
      "('Pclass', -0.7184258754192752)\n",
      "('Prefix', 0.3899117807908663)\n",
      "('SibSp', -0.3787297807952296)\n",
      "('Age', -0.2915178644724024)\n",
      "('Fare', 0.19342430434726862)\n",
      "('Embarked', 0.16377467883922647)\n",
      "('Parch', -0.09580280432420257)\n",
      "\n",
      "Influence function\n",
      "============================\n",
      "('Age', -0.08597749769687653)\n",
      "('SibSp', -0.06355743408203125)\n",
      "('Sex', 0.06121187806129456)\n",
      "('Embarked', -0.03072268341978391)\n",
      "('Fare', 0.028262046227852503)\n",
      "('Parch', -0.024166513482729593)\n",
      "('Pclass', -0.0058502346277236935)\n",
      "('Prefix', -0.002921211222807566)\n"
     ]
    }
   ],
   "source": [
    "features_importance = zip(features, selector.estimator_.coef_[0])\n",
    "sorted_logit_coefs = sorted(features_importance, key=lambda x : abs(x[1]), reverse=True)\n",
    "print(\"\\nLogistic Regression Coeffs\")\n",
    "print(\"============================\")\n",
    "for el in sorted_logit_coefs:\n",
    "    print(el)\n",
    "\n",
    "    \n",
    "print(\"\\nInfluence function\")\n",
    "print(\"============================\")\n",
    "for el in sorted_logit_influence:\n",
    "    print(el)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Average influence on all test points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_feature_influence_pert(test_idx, tf_model):\n",
    "    \n",
    "    num_train = len(tf_model.data_sets.train.labels)\n",
    "    \n",
    "    influences_grad = tf_model.get_grad_of_influence_wrt_input(\n",
    "        np.arange(num_train),\n",
    "        test_idx, \n",
    "        force_refresh=False)\n",
    "\n",
    "    influence_grad_top_features = influences_grad[top_influence_ix, :]\n",
    "\n",
    "    avg_inf_pert = [np.mean(col) for col in influence_grad_top_features.T]\n",
    "    features_avg_inf = zip(features, avg_inf_pert)\n",
    "\n",
    "    sorted_logit_influence = sorted(features_avg_inf, key=lambda x : abs(x[1]), reverse=True)\n",
    "    \n",
    "    print()\n",
    "    for el in sorted_logit_influence:\n",
    "        print(el)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Norm of test gradient: 0.0704115\n",
      "Loaded inverse HVP from tmp/titanic-cg-normal_loss-test-[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49].npz\n",
      "Inverse HVP took 0.0014045238494873047 sec\n",
      "\n",
      "('Sex', -0.04390822562078635)\n",
      "('Age', 0.031846148024002714)\n",
      "('Pclass', 0.03156189372142156)\n",
      "('Prefix', 0.020745747288068137)\n",
      "('Embarked', -0.010099160919586818)\n",
      "('Parch', 0.0076986404756704966)\n",
      "('SibSp', 0.005812543521945675)\n",
      "('Fare', -0.005789404486616453)\n"
     ]
    }
   ],
   "source": [
    "test_idx = list(range(50))\n",
    "get_feature_influence_pert(test_idx, tf_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Measure influence on multiple test points of a similar label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class : Survived\n",
      "===================\n",
      "Norm of test gradient: 0.098934084\n",
      "Loaded inverse HVP from tmp/titanic-cg-normal_loss-test-[0, 3, 4, 5, 6, 8, 9, 15, 16, 17, 21, 22, 27, 31, 34, 36, 37, 38, 50, 51, 53, 55, 57, 58, 59, 61, 62, 69, 71, 72, 73, 77, 81, 82, 86, 87, 90, 96, 97, 98, 102, 106, 108, 109, 110, 112, 113, 118, 121, 122].npz\n",
      "Inverse HVP took 0.0013380050659179688 sec\n",
      "\n",
      "('Pclass', 0.06612778753042221)\n",
      "('Prefix', -0.03319144919514656)\n",
      "('Sex', -0.03147966253260771)\n",
      "('SibSp', 0.0248735008140405)\n",
      "('Fare', 0.022456604739030203)\n",
      "('Age', 0.015206006976465384)\n",
      "('Embarked', -0.004431941298147043)\n",
      "('Parch', 0.0037904473176846904)\n",
      "\n",
      "\n",
      "Class : Died miserably\n",
      "===================\n",
      "Norm of test gradient: 0.13074487\n",
      "Loaded inverse HVP from tmp/titanic-cg-normal_loss-test-[1, 2, 7, 10, 11, 12, 13, 14, 18, 19, 20, 23, 24, 25, 26, 28, 29, 30, 32, 33, 35, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 52, 54, 56, 60, 63, 64, 65, 66, 67, 68, 70, 74, 75, 76, 78, 79, 80, 83].npz\n",
      "Inverse HVP took 0.0010714530944824219 sec\n",
      "\n",
      "('Fare', -0.05614728579918544)\n",
      "('Age', 0.03961841464042663)\n",
      "('Prefix', 0.028393400460481645)\n",
      "('Sex', -0.024337704107165337)\n",
      "('Pclass', 0.012058242782950401)\n",
      "('Parch', 0.00811775413652261)\n",
      "('SibSp', -0.008045802700022857)\n",
      "('Embarked', 0.0009487856645137071)\n"
     ]
    }
   ],
   "source": [
    "# Find indices of sampleztest with Survived == 1\n",
    "#[y_test == 1]\n",
    "\n",
    "\n",
    "sample_ix_pos_y = np.where(y_test == 1)[0].tolist()[:50]\n",
    "sample_ix_neg_y = np.where(y_test == 0)[0].tolist()[:50]\n",
    "\n",
    "\n",
    "print(\"Class : Survived\")\n",
    "print(\"===================\")\n",
    "get_feature_influence_pert(sample_ix_pos_y, tf_model)\n",
    "\n",
    "print(\"\\n\\nClass : Died miserably\")\n",
    "print(\"===================\")\n",
    "get_feature_influence_pert(sample_ix_neg_y, tf_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "### Faire analyse de prior\n",
    "\n",
    "#+ Fare value given Dead / Alive\n",
    "#+ PClass value given dead / alive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAW4AAAD8CAYAAABXe05zAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nQiwePDsW6QAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f8ec06d9ba8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot\n",
    "import pylab\n",
    "\n",
    "x = X_test[:, features.index('Fare')]\n",
    "y = y_test\n",
    "\n",
    "matplotlib.pyplot.scatter(y, x)\n",
    "\n",
    "matplotlib.pyplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "227"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(np.where(X_test[:, features.index('Fare')] <0 )[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYAAAAD8CAYAAAB+UHOxAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nqVEGQJIaZQAkqVEGQJIaZQAkqVH/CwNekaBnmi5iAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f8ec05e50f0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot\n",
    "import pylab\n",
    "\n",
    "x = X_test[:, features.index('Pclass')]\n",
    "y = y_test\n",
    "\n",
    "matplotlib.pyplot.scatter(y, x)\n",
    "\n",
    "matplotlib.pyplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
